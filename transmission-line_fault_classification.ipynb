{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train = pd.read_excel('data_column_train_matlab.xlsx')\n",
    "train_data = train.to_numpy()\n",
    "label = pd.read_excel('data_label_trian_matlab.xlsx')\n",
    "label_data = label.to_numpy()\n",
    "final_data = np.concatenate((train_data,label_data),axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_data = np.random.shuffle(final_data)\n",
    "df = pd.DataFrame(data=final_data)\n",
    "X_train = final_data[:,0:32]\n",
    "Y_train = final_data[:,32:]\n",
    "m = X_train.shape[0]\n",
    "layer_dims = [32,10,5]\n",
    "X_train = (X_train-X_train.mean())/X_train.std()\n",
    "\n",
    "\n",
    "test = pd.read_excel('Test sample set.xlsx')\n",
    "test1 = test.drop(['fault types'],axis=1)\n",
    "\n",
    "test2 = test1.drop(['test results'],axis=1)\n",
    "test3 = test2.to_numpy()\n",
    "X_test = test3[:,1:]\n",
    "Y_test = test3[:,0:1]\n",
    "X_test = (X_test-X_test.mean())/(X_test.std())\n",
    "\n",
    "m = Y_test.shape[0]\n",
    "Y_one_hot = np.zeros((m,5))\n",
    "for i in range(m):\n",
    "    j = int(Y_test[i])\n",
    "    Y_one_hot[i,j-1] = Y_one_hot[i,j-1] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    #layer1=32\n",
    "    #layer2=10\n",
    "    #layer3=5\n",
    "    parameters={}\n",
    "    L = len(layer_dims)\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        parameters['W'+str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
    "        parameters['b'+str(l)] = np.zeros((layer_dims[l],1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def relu(x):\n",
    "    '''\n",
    "    return np.maximum(0,matrix)\n",
    "    '''\n",
    "    # np.where(matrix > 0, matrix, matrix * 0.01)\n",
    "    y1 = ((x > 0) * x)                                                 \n",
    "    y2 = ((x <= 0) * x * 0.01)                                         \n",
    "    leaky_way2 = y1 + y2\n",
    "    return leaky_way2\n",
    "    \n",
    "def sigmoid(matrix):\n",
    "    return 1/(1+np.exp(-matrix))\n",
    "\n",
    "def softmax(matrix):\n",
    "    e_x = np.exp(matrix - np.max(matrix))\n",
    "    return e_x/e_x.sum(axis=0)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \n",
    "    x[x<0] = 0.01\n",
    "    x[x>=0] = 1\n",
    "    \n",
    "    return x\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    df = sigmoid(x)\n",
    "    return df*(1-df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propogation(parameters,X_train):\n",
    "    \n",
    "    cache = {}\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    Z1 = W1@X_train.T + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = W2@A1 + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    cache = {'Z1':Z1,'A1':A1,'Z2':Z2,'A2':A2}\n",
    "    return cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(cache,parameters,X_train,Y_train):\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    A2 = cache['A2']\n",
    "    m = X_train.shape\n",
    "    lambd=0 \n",
    "    cross_entropy = -np.mean(Y_train*np.log(A2.T+np.exp(-8)))\n",
    "    #L2_cost = (np.sum(np.square(W1))+np.sum(np.square(W2)))*(np.divide(lambd,2*m))\n",
    "    cost = cross_entropy # L2_cost\n",
    "    \n",
    "    \n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propogation(cache,parameters,Y_train,X_train):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    lambd =0\n",
    "    \n",
    "    Z1 = cache['Z1']\n",
    "    A1 = cache['A1']\n",
    "    Z2 = cache['Z2']\n",
    "    A2 = cache['A2']\n",
    "    m = X_train.shape[0]\n",
    "    gradients = {}\n",
    "    \n",
    "    dZ2 = A2 - Y_train.T\n",
    "    dW2 = (1/m)*dZ2@A1.T  #(lambd/m)*W2\n",
    "    db2 = (1/m)*np.sum(dZ2,axis=1,keepdims=True)\n",
    "    dZ1 = W2.T@dZ2*relu_derivative(Z1)\n",
    "    dW1 = (1/m)*dZ1@X_train # (lambd/m)*W1\n",
    "    db1 = (1/m)*np.sum(dZ1,axis=1,keepdims=True)\n",
    "    \n",
    "    gradients = {'dZ2':dZ2,'dW2':W2,'db2':db2,'dZ1':dZ1,'dW1':dW1,'db1':db1}\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(layer_dims,parameters,gradients,learning_rate):\n",
    "    L = len(layer_dims)\n",
    "    Vdw = 0\n",
    "    Vdb = 0\n",
    "    Sdw = 0\n",
    "    Sdb = 0\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 0.00000001\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        '''\n",
    "        Vdw = beta1*Vdw +(1-beta1)*parameters['W'+str(l)]\n",
    "        Vdb = beta1*Vdb +(1-beta1)*parameters['b'+str(l)]\n",
    "        Sdw = beta1*Sdw +(1-beta2)*(parameters['W'+str(l)]**2)\n",
    "        Sdb = beta1*Sdb +(1-beta2)*(parameters['b'+str(l)]**2)\n",
    "        \n",
    "        Vdw_corrected = Vdw/(1-beta1**l)\n",
    "        Vdb_corrected = Vdb/(1-beta1**l)\n",
    "        Sdw_corrected = Sdw/(1-beta2**l)\n",
    "        Vdb_corrected = Vdb/(1-beta2**l)\n",
    "        \n",
    "        \n",
    "        parameters['W'+str(l)] = parameters['W'+str(l)] - learning_rate*Vdw_corrected/(math.sqrt(Sdw_corrected)+epsilon)\n",
    "        parameters['b'+str(l)] = parameters['b'+str(l)] - learning_rate*Vdb_corrected/(math.sqrt(Sdb_corrected)+epsilon)\n",
    "        '''\n",
    "        parameters['W'+str(l)] = parameters['W'+str(l)] - learning_rate*parameters['W'+str(l)]\n",
    "        parameters['b'+str(l)] = parameters['b'+str(l)] - learning_rate*parameters['b'+str(l)]\n",
    "        \n",
    "    return parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X,Y,layer_dims,learning_rate,num_iterations):\n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    \n",
    "    for i in range(0,num_iterations):\n",
    "        \n",
    "        cache = forward_propogation(parameters,X)\n",
    "        cost = compute_cost(cache,parameters,X,Y)\n",
    "        gradients = backward_propogation(cache,parameters,Y,X)\n",
    "        \n",
    "        parameters = optimize(layer_dims,parameters,gradients,learning_rate)\n",
    "        costs.append(cost)\n",
    "        if i%10 == 0:\n",
    "            print('Cost after iteration {}: {}'.format(i,cost))\n",
    "            \n",
    "    return parameters,costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_params,costs = model(X_train,Y_train,layer_dims=[32,10,5],learning_rate=0.0075,num_iterations=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(params, X,Y):\n",
    "    accuracy = []\n",
    "    cache = forward_propogation(params,X)\n",
    "    Z1 = cache['Z1']\n",
    "    A1 = cache['A1']\n",
    "    Z2 = cache['Z2']\n",
    "    A2 = cache['A2']\n",
    "    \n",
    "    diff = Y.T - A2\n",
    "    for i in range(diff.shape[1]):\n",
    "        loss = np.max(diff[:,i])*100\n",
    "        acc = 100 - loss\n",
    "        accuracy.append(acc)\n",
    "    \n",
    "    return accuracy,A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "bol,A = predict(final_params,X_test,Y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
